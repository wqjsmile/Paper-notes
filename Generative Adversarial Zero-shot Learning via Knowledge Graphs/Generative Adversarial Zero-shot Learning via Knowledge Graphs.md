# Generative Adversarial Zero-shot Learning via Knowledge Graphs

### Abstract：

零样本学习(ZSL)是处理那些没有标记训练数据的看不见的类的预测。近年来，生成式对抗网络(generative Adversarial Networks, GANs)等生成方法因其较高的准确率和泛化能力而受到广泛的研究。但是，目前使用的类边缘信息仅限于文本描述和属性注释，**缺乏类的语义。**
本文通过将知识图中的丰富语义结合到知识图中，提出了一种新的生成式ZSL方法——KG- gan。具体地说，我们在图神经网络的基础上，从类视图和属性视图两个角度对KG进行编码，考虑到KG的不同语义。通过对每个节点(表示一个可视类别)学习良好的语义嵌入，我们利用GANs为不可见的类合成引人注目的可视特性。根据我们对多个图像分类数据集的评价，KG-GAN能达到比目前最先进的基线更好的性能。

### 1. Introduction：

机器学习通常是在一个封闭的世界假设上运行的:它用一些有标记的样本训练模型，并单独使用在训练阶段出现的类(即见过的类)进行预测。这种限制引起了Zero-shot Learning (ZSL)的研究兴趣，它旨在处理没有任何训练样本的新类(即看不见的类)。
处理这种不可见类的一个直观的想法是利用类的边信息，建立类之间的语义关系，使从可见类获得的知识能够转移到不可见类。例如，在分类中动物图像任务中，边信息通常包含类的视觉特征(例如，人注释的属性或来自外部来源(如Wikipedia)的文本描述)，以及不可见的类如何与可见的类相关(例如，分类中的类层次结构)，如图1所示。

![image-20210615103231865](https://i.loli.net/2021/06/15/1dbSDMEgRl8Ixar.png)

ZSL通常分为两种范式。一种是基于映射[14,19,13,24]。它学习一个通用的映射函数，将视觉特征和/或语义特征映射到相同的潜在空间，然后进行最近邻搜索来预测类标签。然而，由于类级语义描述产生非视觉特征，因此在这两个空间之间架起**语义鸿沟**并不是一项简单的任务。此外，最近邻搜索还存在枢纽问题，即映射元素的邻域偏向于一些枢纽向量，将正确的标签推到邻居列表[21]下。

另一种ZSL范式是在生成模型(如生成对抗网络)的基础上发展起来的[26,28,9,16,29,22]。这些方法利用类的边信息来为不可见的类合成样本(或特征)，避免了空间映射的需要，从而也避免了语义鸿沟问题。这种方法将ZSL问题转化为传统的监督学习问题，可以用现有的各种方法灵活处理。此外，生成的不可见样本(或特征)可以缓解训练对可见类的偏倚，避免上述枢纽问题。

然而，大多数生成方法都是建立在一类边缘信息(如属性注释、分类结构或文本描述)之上的。因此，它们通常会在没有足够变化的情况下生成较少辨别力的样本(或特征)。
以动物类的标注属性为例，当我们使用“stripe(条纹)”属性为斑马类生成样本时，另一个显著不同的也标注了“stripe(条纹)”的老虎也可能得到与斑马类相似特征的合成样本(即域偏移问题[6])，特别是当tiger缺少其他具有代表性的属性注释时。分类结构描述了分类中的类间关系，如马属于马科，虎属于大猫科。然而，它将产生较少的鉴别样本，兄弟类可能看起来很不同，如马和斑马。文本描述容易收集，但由于歧义和不相关的词和短语，容易引入大量的噪声。

在本文中，我们提出了一个包含上述所有语义信息的知识图(KG)，可以提高ZSL性能。
为此，我们首先构建带有两个视图的KG:用于分类结构的类视图和用于属性注释的属性视图，并将其与从文本描述中学习到的类名单词向量一起嵌入到图神经网络(gnn)的向量空间中。然后，我们提出了KG- gan一个新的生成ZSL框架，利用上述KG嵌入，以及生成对抗网络(GANs)，它综合了每个类别的区分性视觉特征。与之前使用正则化器或复杂网络的生成方法不同，我们的框架采用了基本的GAN模型，没有任何附加。
我们的主要贡献如下:

- 据我们所知，KG- gan是最早利用生成ZSL中以KG为代表的正式丰富语义的方法之一。KG描述了ZSL类的不同方面，促进了可见类和不可见类之间的知识转移。
- 我们开发了图神经网络来学习语义上有意义的类嵌入，从而研究类语义如何影响ZSL中的特征迁移。
- 在这两个基准上的实验表明，生成的特征对可见类和不可见类都是相当有效的。
  与包括生成和非生成ZSL方法在内的最先进的基线相比，已经取得了有希望的结果。

### 2. Related Work

#### 2.1 基于映射的ZSL vs .生成ZSL

在基于映射的零样本学习文献中，[5,15,19,13]等方法试图将视觉特征映射到语义空间，通过计算类嵌入上的最近邻来寻找最相似的类。然而，这些方法往往会加剧枢纽问题[23]，因为一些视觉特征被映射到某个类的语义空间中的一个点，导致不相关的点(hub)成为最近的邻居(即正确的标签)的概率增加。其他一些将类嵌入映射到视觉空间的方法也被提出来抑制这一问题[4,27,2]，其中，看不见的类的特征是通过在语义空间中基于类的关联转移看到的类的特征来学习的。然而，特征迁移受到两个空间映射的限制，严重依赖类的语义关系，这可能会受到语义鸿沟问题的影响。此外，未知分类器的学习只依赖于已知类的样本，这可能在预测过程中对已知类有很强的偏倚。

与基于映射的ZSL不同的是，生成式零样本学习直接合成不可见的样本(或特征)与随机噪声，这些随机噪声受类边缘信息的约束。例如，Zhu et al.[28]利用GANs作为生成模型，将维基百科文章中的类的文本描述作为输入，为这些类生成视觉特征，并使用全连接层来减少文本噪声。此外，他们还提出了一种视觉枢轴正则化，以保持生成特征的类间区分。Huang等人[9]引入了一种生成器来合成带有类嵌入的样本特征，并引入了一种回归器来将生成的特征投影到相应的类嵌入中，而鉴别器则用来评估视觉特征和类嵌入的接近程度。然而，这些方法大多依赖于工程正则化器或辅助网络来保证生成样本的质量。他们很少考虑类语义的有效性。
本文利用含有丰富类语义的知识图来增强ZSL中的特征生成，使合成的数据具有较强的鉴别性。

#### 2.2 基于KG的ZSL

有一些研究利用KG来增强基于映射的ZSL。例如，Wang等人[24]提出了一个KG描述分类法中的类层次关系，并利用这种层次关系预测不可见类的分类器，其中，图卷积网络(Graph Convolutional Network, GCN)用于将特征从可见的分类器转移到不可见的分类器。
该方法及其派生的[10]有以下问题:1)所使用的KG是同构的，其中类语义受到限制，特别是用于区分那些兄弟类;【区分兄弟类比较难】2)模型只使用可见的样本进行训练，在预测过程中对可见的类别有很强的偏向性，特别是在广义ZSL场景中，预测既涉及可见的类别标签，也涉及不可见的类别标签。【确实】相比之下，我们从分类中的类结构、类属性和类名词向量等侧面信息中使用了语义更丰富的KG，而生成式解决方案为未见类生成训练样本，避免了广义ZSL场景中的偏差问题。据我们所知，目前还没有在生成ZSL中加入这种KG的作品。

### 3. Methodology

![image-20210615141559198](https://i.loli.net/2021/06/15/vfrwL9Iu81BKtDX.png)

在本节中，我们将详细介绍KG-GAN，如图2所示。
首先利用无监督图神经网络(graph Auto-Encoders, GAEs)嵌入知识图，知识图包括两种视图:类视图(class view)和属性视图(attribute view)，类视图建模类之间的层次关系，属性视图建模人类标注的视觉特征。简单地说，我们首先根据KG中每个类对应的节点得到一个嵌入向量。其次，采用具有Wasserstein距离[1]和分类损失的GAN进行特征生成。它包括(i)一个生成器，用于从以类嵌入为条件的随机噪声中合成视觉特征，(ii)一个鉴别器，用于将生成的特征与真实的特征区分开来，以及(iii)一个监督分类损失，用于生成的特征鉴别类。需要注意的是，为了更高的精度和计算效率[26]，我们生成了图像特征而不是原始图像，并且我们采用了一些预先训练好的CNN模型(如ResNet)来提取图像特征。最后，通过训练有素的生成器，我们可以为每个看不见的类合成引人注目的图像特征并为它训练一个softmax分类器。

#### 3.1 初期 ZSL和KG

我们首先定义ZSL问题如下：

$D_{tr}=\left \{ \left ( x,y \right )|x \in X_s,y \in Y_s \right \}$表示ZSL的训练集，其中x是训练图片的CNN特征，y是(考虑可见类)$Y_s$的类别标签。测试集表示为$D_{te}=\left \{ \left ( x,y \right )|x \in X_u,y \in Y_u \right \}$,其中$y_u$是不可见类集与$y_s$没有交集。在训练过程中，$D_{tr}$和$Y_u$用来给学习不可见类别分类器。我们研究了两种预测过程：ZSL和GZSL。前者预测测试样本$X_u$，而后者预测测试样本和训练样本$X_u$和$X_s$。

知识图谱作为上述训练过程额外的输入。知识图谱KG包含一系列类节点$C=\left \{ c_1, c_2,...c_n \right \}$,和一系列属性节点$A=\left \{ a_1, a_2, ..., a_m \right \}$.KG也包含两部分，类别视图$G^c$和属性视图$G^a$.类别是有类别节点构建，关系(边)表示分类法中定义的类的层次关系。属性视图是一个由类节点和属性节点组成的异构二部图，每个类节点与一系列通过“hasAttribute”关系边标注的属性节点相连接。在KG嵌入中，我们学习了一个函数g(·)来将每个类节点编码为一个向量，称为类嵌入。对于$Y_s \cup Y_u$中的每个类y，我们可以分别根据类视图和属性视图学习两个嵌入$g^c(y)$和$g^a(y)$。为方便起见，第i类的嵌入用下标表示，即$g^c_i$和$g^a_i$。

#### 3.2 KG类别嵌入

Graph Auto-Encoder (GAE)[12]是一种基于变分自编码器(VAE)[11]的图结构化数据无监督学习方法。它以图卷积网络(GCN)为编码器，以内积为解码器，使图节点的潜在表示被学习，这就是我们想要的类嵌入。

**图编码器** GCN的工作是通过一系列的图卷积在图中的节点之间传播信息，并捕获图结构化数据的依赖性。因此，我们使用GCN来编码所提议的KG中反映的类间关系和类-属性相关性。

在GCN的每一层中，卷积运算通过聚合图中定义的邻近节点的向量来计算节点的向量表示，并将其更新到下一层。将卷积运算逐层叠加后，可以输出图节点最后一层的潜在嵌入。

给定KG G，我们首先在图$G^c$上应用GCN，并从类视图中计算类节点的嵌入。对于第i类，其第k层向量表示为:

![image-20210615150750871](https://i.loli.net/2021/06/15/CewOmFMhDQ2RHrL.png)

其中$N_i$是第i类的相邻类集，$W^c_k$和$B^c_k$分别表示分层可训练权矩阵和类视图中的偏置项。
最后一层输出class-view中i类别的潜在嵌入:$g^c_i = g^c_{i,K}$.

然后使用另一个在$G^a$图上的GCN，其中类别节点由其邻近属性节点的向量聚合，类别i的第l层向量表示为：

![image-20210615151316091](https://i.loli.net/2021/06/15/TNBMbvjR35JGuIK.png)

其中$M_i$是类别i的邻居属性，$g_{j,l-1}^a$是属性j在l-1层的属性表示。$W_l^a$和$B_l^a$表示属性视图中分层权重和偏向。我们也从输出层$g^a_i = g^a_{i,K}$获得属性视图中类别i的潜在嵌入.

最后，我们拼接类别视图嵌入$g^c_i$和属性视图嵌入$g^a_i$，从而形成 节点i的类别嵌入：

![image-20210615151906137](https://i.loli.net/2021/06/15/AFodHGsmYWnVuX4.png)

为了丰富图节点的语义，我们用类名和属性名的词嵌入来初始化节点表示，这些词嵌入是在维基百科文章的跳跃图模型上训练的。这些嵌入作为两个GCNs的输入。

**图解码器** 为了保持通过关系边连接的两个节点之间的关系，解码器在潜在空间中进行这些连接节点之间的接近度计算。具体地说，对于每个连接节点对，我们在它们的潜在嵌入之间进行内积。根据图中观察到的链接，我们可以通过最小化以下损失函数来优化模型：

![image-20210615152049997](https://i.loli.net/2021/06/15/PTrUghEvc3jLBJ8.png)

σ(·)是sigmoid激活函数,$(i, j)$是一对关联节点,节点$j'$不与节点$i$相连。Ω是观察到的(积极的)链接设置,Ω−负集,其中包括所有链接节点对(Ω的补充)。w是权重，计算积极的和消极的链接数量的比率。这样，我们使有链接的节点彼此靠近，而没有链接的节点彼此远离，优化了图节点的潜表示学习。

#### 3.3 GAN的特征生成

通过学习语义上有意义的类嵌入，我们学习了一个生成器G，它以类嵌入G (y)和从高斯分布N(0,1)采样的随机噪声向量z作为输入，合成了一类y的CNN图像特征$\hat{x}$。G的损失定义为:

![image-20210615152726466](https://i.loli.net/2021/06/15/gCLuEGW5e2hod8p.png)

其中，$\hat{x}= G(z,g(y))$.损失函数的第一项是Wasserstein损失[1]，第二项是用于对综合特征进行分类的监督分类损失。λ为对应的权重系数。

然后，鉴别器D将合成特征$\hat{x}$和从y的训练图像中提取的真实特征x作为输入，其损失函数可表示为:

![image-20210615153035091](https://i.loli.net/2021/06/15/gIsepCbirhGKDYl.png)

其中，前两项近似实特征和合成特征分布的Wasserstein距离，最后一项是梯度惩罚，使D的梯度具有单位范数(即在[7]中提出的Lipschitz约束)，其中$\tilde{x}=\varepsilon x + \left ( 1-\varepsilon  \right )\hat{x}$,$\varepsilon \sim U(0,1)$,并且$\beta $是相应的权重系数.

GAN通过极大极小策略进行优化，使G的损失最小化，但使d的损失最大化。我们还注意到，在训练过程中，发生器和鉴别器都与类嵌入相结合。这是一种典型的引入外部信息来指导GANs训练的条件式GANs[18]方法，与基于类的边信息合成视觉特征的生成ZSL方法完全一致。此外，不同于大多数提出的生成方法，引入辅助正则化或网络来确保生成特征的类间区分，我们使用基本的Wasserstein GAN和分类损失实现我们的特征生成模块。**我们的KG- gan的核心是在KG的不同和特征类语义的基础上产生有区别的视觉特征。**

#### 3.4 不可见类的softmax分类器

在训练阶段，我们使用所看到的类的图像特征和类嵌入来训练GAN模型。一旦经过良好的训练，KG-GAN能够从随机噪声中合成不可见类的视觉特征及其相应的类嵌入，因为这些不可见类在语义上与知识图中可见的类相关。因此，用合成的看不见的数据$\hat{X}_u$，我们可以为每个不可见的类学习一个softmax分类器，并对其测试样本进行分类。分类器的优化方法如下:

![image-20210615153758269](https://i.loli.net/2021/06/15/QAoKjScFkUEsebV.png)

其中，X表示训练图片的特征，Y表示预测的标签集，$\theta$是训练参数，
$$
P(y|x;\theta ) = \frac{exp(\theta ^T_y x)}{\sum _i^|Y| exp(\theta _i^Tx)}​.
$$
考虑不同的预测设置，在ZSL任务中$X = \hat{X_u}$,在GZSL任务中$X = X_s \cup \hat{X_u}$,对应的y设置为$Y_u$和$Y_s \cup Y_u$.

### 4.Experiments

我们现在对图像分类任务进行实验来评估我们提出的KG-GAN。首先，我们将KG- gan与ZSL和GZSL设置中最先进的基线进行比较，然后我们探索来自KG的丰富语义是否比文本描述(即类词向量)等其他边缘信息更有效。我们还分析了从不同的KG观点中学习到的类嵌入的影响，并验证了将语义嵌入与基本的GAN结合比在这些基线中广泛使用的附加正则化结合要好。

#### 4.1 实验设置

**数据集** 我们从广泛使用的基准ImageNet[3]中提取评估数据。它是一个大规模的图像分类数据集，总共包含21K个类，这些类作为分类结构分层关联存储在WordNet[17]中。在ImageNet上进行预测是一项挑战，因为有1000个类被视为有训练样本的类，但大约有20K个没有训练样本的类被视为看不见的类。此外，ImageNet包含细粒度数据集和粗粒度数据集的集合。来自细粒度子集的类通常被分组成不同的族，例如不同的交通工具类型和不同的鸟类类型。

为了研究ZSL中类之间的语义关系，我们从细粒度子集中提取了两个数据集进行评估。一个是针对特定领域的动物分类(即ImNet-A)，另一个是针对一般对象分类(即ImNet-O)。数据集分割遵循[25]中最初提出的看不见的分割方法。据统计，ImNet-A中有25个可见的类，每个类包含约1300幅图像作为训练样本，还有55个未见的类，它们是所见类的祖先、后代或兄弟姐妹，但没有训练样本。类似地，在ImNet-O中，10个类被视为可视类，25个被视为不可见类。数据集的详细信息如表1所示。

![image-20210615155040358](https://i.loli.net/2021/06/15/QUI1F8OPJAndEoG.png)

**知识图谱结构** 我们采用WordNet原有的分类结构作为知识图的骨干，其中ImageNet类通过子类关系进行抽取，并相互连接，如图2所示。此外，由于ImageNet类的属性不可用，我们邀请志愿者手动注释这些类的属性。具体来说，对于每个类，注释器被要求从属性list5中分配3个∼6个属性，其中包含25个图像作为引用。每个类都由3个∼4个志愿者评审，我们将注释者之间的共识作为最终的注释。最后，我们注释了76个ImNet-A类的属性和38个ImNet-O类的属性。我们将这些属性添加到知识图中，并通过hasAttribute关系将它们与相应的类节点连接起来。建造KG的细节见补充材料。

**基线和指标** 我们采用经典的和最先进的ZSL方法作为基线。其中，design[5]、CONSE[19]和SAE[13]是将视觉特征映射到语义空间的方法，通过计算类名词向量上的最近邻来预测测试样本的标签;像SYNC [2]， GCNZ[24]和DGP[10]这样的方法提出了将类名单词向量映射到视觉空间中来学习不可见类的分类器。注意，GCNZ和DGP是两种使用KGs的ZSL方法，而GAZSL[28]、LisGAN[16]和ZSL- abp[29]是生成方法，它们根据类名单词向量生成视觉特征(即利用文本描述)。为了进行公平比较，我们在我们提出的数据集上重新评估这些基线，并在相同的设置中重新实现所有的方法6。

在[25]之前的工作之后，我们用Hit@k评估基线和KG-GAN，即top k预测标签达到ground-truth标签的样本比例。考虑到ImageNet中未显示类的样本数量不平衡，我们对每个类分别计算Hit@k并取其平均值作为结果。在ZSL测试设置中，结果是每个看不见的类Hit@k的平均值。而在GZSL中，我们计算谐波平均值H = (2 Hs Hu)/(Hs +Hu)，其中$H_s$和$H_u$分别代表可见类和未可见类的平均每类Hit@k。值得注意的是，我们在ZSL设置中将k设为1,2,5，在GZSL设置中将k设为1。

**实现** 我们使用性能良好的CNN模型ResNet101[8]来提取2048维的图像视觉特征，这些特征是在ImageNet中看到的类的样本上预先训练的。对于类名词向量，我们使用Changpinyo等人提供的预训练词嵌入，他们在维基百科语料库上训练跳gram模型，为每个类学习一个500维的词向量。**由于没有预先训练好的属性名称词的嵌入，我们使用Glove[20]模型在维基百科语料库上进行训练。**

在我们的KG-GAN中，GAE的编码器采用了两层GCN。对两个视图中的KG进行编码后，我们为每个类获得一个100维的嵌入类。我们还将噪声矢量z的维数设为100。GAN的生成器和鉴别器都是用两个完全连接层,有4个,096隐藏单元和输出2,048维合成特征,鉴别器也有4,096个隐藏单元和输出二维向量来表示输入特性是否真实。在GAE模块训练中，学习率设置为0.001，而在GAN训练中，学习率设置为0.0001。我们将分类损失的权重λ设为0.01，梯度惩罚的权重β设为10。

#### 4.2 和基线的比较

![image-20210615160401334](https://i.loli.net/2021/06/15/EstK5P1p6IwOqCg.png)

**ZSL 设置** 我们首先在表2中报告了零样本学习结果。可以看出，我们的方法在Hit@1和Hit@2两个数据集上取得了最好的结果，在Hit@5上也取得了最先进的结果。特别是在ImNet-A上，性能比最先进的DGP在Hit@1上提高了6.17%，在Hit@2上提高了3.32%。在ImNet-O上，Hit@1和Hit@2的性能分别提高了3.42%和4.83%。人们普遍认为Hit@1和Hit@2相对更重要[25]，因为这两个指标表明模型能够更准确地预测标签的正确位置。

从这些结果可以看出，除了GCNZ和DGP之外，这些生成式方法的性能优于大多数基于映射的方法。这些基于映射的方法通过语义空间和视觉空间之间的相互映射来对未见测试样本进行预测，而生成方法则直接根据类语义生成未见类的训练样本。在基于映射的方法中，我们注意到，从视觉空间映射到语义空间的方法(如design、CONSE和SAE)的性能远远低于从语义空间映射到视觉空间的方法(如SYNC)。这说明由于hub度问题，视觉空间到语义空间的映射竞争较弱。GCNZ和DGP是基于kg的方法，以类的层次关系作为辅助语义，辅助类名词向量到视觉特征的映射，性能优于其他仅以类名词嵌入作为类语义的基线。我们可以得出结论，随着类语义的丰富，ZSL的性能可以得到显著的提高，特别是在生成ZSL模型中引入包含各种类边信息的知识图，取得了最好的效果。

**GZSL设置** 

![image-20210615161355079](https://i.loli.net/2021/06/15/wzvSDR1M4cuG8ki.png)

我们进一步评估了表3中广义ZSL的结果，可以得出与表2相似的结论。我们的KG-GAN方法在预测未见测试样本(Hu)和平均值(H)方面表现优于列示方法，说明我们的KG-GAN具有更好的泛化能力。然而，我们注意到基于映射的方法的性能显著下降。像CONSE和SYNC这样的方法甚至会在两个数据集上降至0.00。这说明了这些方法在预测过程中对可见类有很强的偏倚，即模型倾向于预测可见类集中带有标签的不可见测试样本，即使标签空间中包含不可见类，这可能是由于这些模型对已见类的训练数据过拟合，不能很好地概括未见类。相比之下，这些生成方法削弱了这一趋势，并提高了未见类的性能。我们还发现，虽然我们的框架没有实现看到的测试样本(Hs)的预测的最佳结果，但它仍然实现了最先进的性能。这促使我们探索优化算法，以正确预测未见测试样本，并保持对已见类的高准确度。

#### 4.3 类别语义分析

![image-20210615161515624](https://i.loli.net/2021/06/15/WNd42efFDVgkzqv.png)

为了验证基于知识图的类语义的优越性，我们将基于映射方法的类嵌入(即类名词向量)替换为在我们的模型(参见3.2节)中学习到的类嵌入，并对这些模型进行再训练，以预测未显示的测试样本。在图3中，我们报告了基于映射的基线和我们的KG-GAN的比较结果。我们发现，无论模型的映射方向是什么，所有基线的性能都有显著的改进。所有这些方法在两个数据集上在ZSL设置下均有超过10%的增量，在GZSL设置下均有超过6%的增量，尤其是SYNC，其谐波平均值在ImNet-A上从0%增加到21.48%，在ImNet-O上从0%增加到20.02%。
另一方面，由于有限的类语义，我们的KG-GAN将类名单词嵌入作为输入，预期性能会更差。我们还增强了以前基于kg的方法(即GCNZ和DGP)中的类语义。具体来说，我们将我们的方法中产生的属性节点添加到KG中，它们用于学习不可见的分类器。结果表明，以DGP为例，在ZSL设置下，其性能在ImNet-A和ImNet-O上分别提高了3.21%和2.72%，在GZSL设置下，其性能在ImNet-A和ImNet-O上分别提高了5.26%和0.93%。综上所述，该知识图包含了关于可见类和不可见类的丰富语义，对于ZSL问题具有很大的优势，从中学到的类嵌入可以在一定程度上弥补ZSL模型的不足。

#### 4.4 KG视图的影响

在本小节中，我们分析了KG的不同视角对学习类嵌入的贡献。具体来说，我们分别将不同视图的类嵌入，即类视图类嵌入$g^c$(记为$g^c$)和属性视图类嵌入$g^a$(记为$g^a$)作为KG-GAN的输入来合成视觉特征，并对不同视图生成的特征进行质量评估。我们在表4的最后一行给出了结果，从中可以看出，属性视图类嵌入比ZSL和GZSL设置中的类视图类嵌入具有更好的性能。性能更高的原因可能是(i)属性注释描述了类对象的可辨别的视觉特征，使得ZSL模型能够区分不同的类别，特别是外观相似的类别;(ii)我们提出的数据集是细粒度的，其中包含了一些在分类上差异不明显的兄弟类，如图1中的horse和zebra，如果只考虑类视图类的嵌入，很难区分这些类的测试样本。然而，当结合这两种类嵌入时，我们可以获得令人印象深刻的预测结果，说明我们的知识图中不同的类语义视图是有意义的和互补的。

4.5 正则化器或类语义?

在生成ZSL方法的文献中，有些方法将精心设计的正则化器与GAN结合起来，以提高合成特征的质量。例如，在我们的基线中，GAZSL利用可视化枢轴将每个类的生成特征的平均值规范化为真实特征分布的平均值; LisGAN将生成的样本规范化，使其更接近真实样本，代表了真实样本的多视角原型表征。相比之下，我们的KG-GAN只依赖于丰富的类语义来合成类的可鉴别特征，而没有任何正则化。为了比较正则化器和类语义在生成高质量样本方面的有效性，我们将两视图KG(即GC和GA)学习到的两个类嵌入输入到上述生成模型中，并分析ZSL和GZSL设置下的预测结果。如表4所示，我们结合GC和GA的方法优于结合GC (GA)和正则化器的基线。这表明对于生成ZSL模型，丰富的类语义可能比优化的正则化器更重要。

![image-20210615162245312](https://i.loli.net/2021/06/15/mxHI1FM8EjvOSZa.png)

### 5.Conclusions

本文提出了一种基于生成式对抗网络(generative Adversarial Networks)的生成式零距离学习(generative Adversarial Networks)中利用两种视图的知识图作为类语义来合成样本特征。
我们的方法KG-GAN在来自ImageNet的不同数据集上取得了很有前景的结果，在无生成ZSL和生成ZSL方面都超过了目前的水平。它利用Graph Auto-Encoder学习有语义意义的类嵌入，在语义上优于传统的类名词向量和类层次关系，在生成有区别性的视觉特征上优于精心设计的正则化。这激励我们去探索更多潜在的侧信息，构建更丰富的知识图来解决ZSL的问题，特别是改进可见类和不可见类的性能。